#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ„ç¹”åŒ–æœå°‹ç³»çµ± - å°‡æ‰€æœ‰æœå°‹çµæœçµ±ä¸€ç®¡ç†åˆ°å°ˆé–€è³‡æ–™å¤¾
"""

import json
import csv
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict
from chat_analyzer import get_chat_analyzer
from keyword_generator import get_keyword_generator
from database_manager import get_database_manager
from customer_integration_analyzer import get_customer_integration_analyzer

# è¨­å®šçµ±ä¸€æ—¥èªŒé…ç½®
from logging_setup import setup_logging
from progress_tracker import progress_tracker
logger = setup_logging(__name__)

# ä¸»è¦æœå°‹çµæœè³‡æ–™å¤¾ - ä½¿ç”¨çµ•å°è·¯å¾‘ç¢ºä¿åœ¨APIç’°å¢ƒä¸­æ­£ç¢ºä¿å­˜
current_dir = Path(__file__).parent.parent  # ä¸Šä¸€å±¤ç›®éŒ„(988code)
SEARCH_RESULTS_DIR = current_dir / "customer_search_results"

def setup_search_results_directory():
    """åˆå§‹åŒ–æœå°‹çµæœä¸»è³‡æ–™å¤¾çµæ§‹"""
    print(f"æ­£åœ¨å‰µå»ºæœå°‹çµæœç›®éŒ„: {SEARCH_RESULTS_DIR.absolute()}")
    SEARCH_RESULTS_DIR.mkdir(exist_ok=True)
    
    # å‰µå»ºå­è³‡æ–™å¤¾çµæ§‹
    (SEARCH_RESULTS_DIR / "by_date").mkdir(exist_ok=True)
    (SEARCH_RESULTS_DIR / "by_product").mkdir(exist_ok=True)
    (SEARCH_RESULTS_DIR / "reports").mkdir(exist_ok=True)
    
    # å‰µå»ºä¸»èªªæ˜æª”æ¡ˆ
    readme_content = """# å®¢æˆ¶æœå°‹çµæœç®¡ç†ç³»çµ±

## è³‡æ–™å¤¾çµæ§‹

```
customer_search_results/
â”œâ”€â”€ by_date/           # æŒ‰æ—¥æœŸåˆ†çµ„çš„æœå°‹çµæœ
â”œâ”€â”€ by_product/        # æŒ‰ç”¢å“åˆ†çµ„çš„æœå°‹çµæœ
â”œâ”€â”€ reports/           # ç¶œåˆåˆ†æå ±å‘Šèˆ‡æœå°‹ç´¢å¼•
â”‚   â””â”€â”€ search_index.json  # æœå°‹ç´¢å¼•æª”æ¡ˆ
â””â”€â”€ README.md          # æœ¬èªªæ˜æª”æ¡ˆ
```

## ä½¿ç”¨èªªæ˜

1. **by_date/**: æ¯æ¬¡æœå°‹éƒ½æœƒåœ¨æ­¤å»ºç«‹æ™‚é–“æˆ³è¨˜è³‡æ–™å¤¾
2. **by_product/**: åŒä¸€ç”¢å“çš„æœå°‹çµæœæœƒå»ºç«‹ç”¢å“å°ˆç”¨è³‡æ–™å¤¾
3. **reports/**: è·¨ç”¢å“æˆ–è·¨æ™‚é–“çš„åˆ†æå ±å‘Šï¼ŒåŒ…å«æœå°‹ç´¢å¼•æª”æ¡ˆ
   - **search_index.json**: æ‰€æœ‰æœå°‹çš„ç´¢å¼•å’Œå¿«é€ŸæŸ¥è©¢

## æª”æ¡ˆæ ¼å¼

- **JSON**: å®Œæ•´çµæ§‹åŒ–æ•¸æ“šï¼Œç¨‹å¼å¯è®€
- **CSV**: Excel å¯é–‹å•Ÿçš„è¡¨æ ¼æ ¼å¼
- **TXT**: äººé¡å¯è®€çš„çµ±è¨ˆå ±å‘Š

## æœå°‹æ­·å²

æ‰€æœ‰æœå°‹è¨˜éŒ„éƒ½æœƒè¢«ä¿å­˜ï¼Œå¯é€éç´¢å¼•æª”æ¡ˆå¿«é€ŸæŸ¥æ‰¾ç‰¹å®šæ™‚é–“æˆ–ç”¢å“çš„æœå°‹çµæœã€‚
"""
    
    readme_file = SEARCH_RESULTS_DIR / "README.md"
    if not readme_file.exists():
        readme_file.write_text(readme_content, encoding='utf-8')
    
    return SEARCH_RESULTS_DIR

def load_search_index():
    """è¼‰å…¥æœå°‹ç´¢å¼•"""
    # ç¢ºä¿ reports è³‡æ–™å¤¾å­˜åœ¨
    reports_dir = SEARCH_RESULTS_DIR / "reports"
    reports_dir.mkdir(exist_ok=True)
    index_file = reports_dir / "search_index.json"
    if index_file.exists():
        with open(index_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"searches": [], "products": {}, "statistics": {"total_searches": 0, "total_matches": 0}}

def save_search_index(index_data):
    """ä¿å­˜æœå°‹ç´¢å¼•"""
    # ç¢ºä¿ reports è³‡æ–™å¤¾å­˜åœ¨
    reports_dir = SEARCH_RESULTS_DIR / "reports"
    reports_dir.mkdir(exist_ok=True)
    index_file = reports_dir / "search_index.json"
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2, default=str)

def organized_complete_search(product_name: str):
    """çµ„ç¹”åŒ–çš„å®Œæ•´æœå°‹ç³»çµ±"""
    
    # é–‹å§‹é€²åº¦è¿½è¹¤
    task_id = f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    progress_tracker.start_task(task_id, product_name)
    
    try:
        # åˆå§‹åŒ–æœå°‹çµæœç›®éŒ„
        setup_search_results_directory()
        progress_tracker.update_step(1, "ç³»çµ±åˆå§‹åŒ–", "æœå°‹çµæœç›®éŒ„åˆå§‹åŒ–å®Œæˆ")
        
        # å‰µå»ºæ™‚é–“æˆ³è¨˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # å‰µå»ºæŒ‰æ—¥æœŸå’ŒæŒ‰ç”¢å“çš„è³‡æ–™å¤¾
        date_folder = SEARCH_RESULTS_DIR / "by_date" / f"search_{timestamp}"
        
        # æ¸…ç†ç”¢å“åç¨±ä¸­çš„ Windows ä¸æ”¯æ´å­—ç¬¦
        clean_product_name = (product_name
                             .replace('/', '_')
                             .replace('\\', '_') 
                             .replace('*', '_')
                             .replace('?', '_')
                             .replace('"', '_')
                             .replace('<', '_')
                             .replace('>', '_')
                             .replace('|', '_')
                             .replace(':', '_')
                             .replace('(', '_')
                             .replace(')', '_')
                             .replace('[', '_')
                             .replace(']', '_')
                             .replace('Â±', '_')
                             .replace('%', '_')
                             .replace(' ', '_'))
        
        product_folder = SEARCH_RESULTS_DIR / "by_product" / clean_product_name
        
        date_folder.mkdir(exist_ok=True)
        product_folder.mkdir(exist_ok=True)
        
        print(f"æœå°‹çµæœå°‡ä¿å­˜åˆ°:")
        print(f"   æŒ‰æ—¥æœŸ: {date_folder}")
        print(f"   æŒ‰ç”¢å“: {product_folder}")
        print(f"æ­£åœ¨é€²è¡Œå®Œæ•´æœå°‹ç”¢å“: {product_name}")
        
        # ç²å–é—œéµè©
        print("ç”Ÿæˆé—œéµè©...")
        progress_tracker.update_step(2, "é—œéµè©ç”Ÿæˆ", "æ­£åœ¨ä½¿ç”¨AIç”Ÿæˆç”¢å“ç›¸é—œé—œéµè©...")
        generator = get_keyword_generator()
        keywords = generator.generate_keywords_for_product(product_name)
        print(f"ç”Ÿæˆäº† {len(keywords)} å€‹é—œéµè©")
        progress_tracker.update_step(3, "é—œéµè©ç”Ÿæˆå®Œæˆ", f"æˆåŠŸç”Ÿæˆ {len(keywords)} å€‹æœå°‹é—œéµè©")
    
        # é€²è¡Œå®Œæ•´æœå°‹
        print("é–‹å§‹å®Œæ•´æœå°‹æ‰€æœ‰æª”æ¡ˆ...")
        progress_tracker.update_step(4, "æª”æ¡ˆæœå°‹ä¸­", "é–‹å§‹æƒææ‰€æœ‰èŠå¤©è¨˜éŒ„æª”æ¡ˆ...")
        analyzer = get_chat_analyzer()
        
        all_results = []
        csv_files = list(analyzer.chat_dir.glob("*.csv"))
        total_files = len(csv_files)
        
        print(f"å°‡æœå°‹ {total_files} å€‹æª”æ¡ˆ...")
        progress_tracker.add_message(f"ç™¼ç¾ {total_files} å€‹èŠå¤©è¨˜éŒ„æª”æ¡ˆå¾…è™•ç†")
        
        for i, csv_file in enumerate(csv_files, 1):
            # æ›´é »ç¹çš„é€²åº¦æ›´æ–°ï¼šæ¯50å€‹æª”æ¡ˆæ›´æ–°ä¸€æ¬¡å‰ç«¯é€²åº¦
            if i % 50 == 0 or i == total_files or (total_files - i < 10):
                step_message = f"æœå°‹é€²åº¦: {i}/{total_files} ({i*100/total_files:.1f}%)"
                progress_tracker.update_step(4, "æª”æ¡ˆæœå°‹ä¸­", step_message)
                print(f"   é€²åº¦: {i}/{total_files} ({i*100/total_files:.1f}%)")
        
            try:
                chat_records = analyzer._read_chat_file(csv_file)
                
                for record in chat_records:
                    matches = analyzer._search_keywords_in_message(keywords, record['content'])
                    
                    if matches:
                        for match in matches:
                            result = {
                                'product_name': product_name,
                                'customer_name': record['customer_name'],
                                'customer_id': record.get('customer_id'),  # åŠ å…¥ customer_id
                                'message_content': record['content'],
                                'message_date': record['date'],
                                'date_str': record['date_str'],
                                'sender_name': record['sender_name'],
                                'matched_keyword': match['keyword'],
                                'match_type': match['match_type'],
                                'match_score': match['confidence'],
                                'file_source': record['file_source']
                            }
                            all_results.append(result)
            
            except Exception as e:
                print(f"âš ï¸ éŒ¯èª¤è™•ç†æª”æ¡ˆ {csv_file.name}: {e}")
                continue
        
        print(f"\næœå°‹å®Œæˆï¼æ‰¾åˆ° {len(all_results)} å€‹åŒ¹é…çµæœ")
        progress_tracker.update_step(5, "è³‡æ–™åˆ†æ", f"æª”æ¡ˆæœå°‹å®Œæˆï¼Œæ‰¾åˆ° {len(all_results)} å€‹åŒ¹é…çµæœ")
        
        # æ–°å¢ï¼šè³¼è²·æ­·å²æª¢æŸ¥å’Œå®¢æˆ¶åˆ†é¡
        print("æª¢æŸ¥å®¢æˆ¶è³¼è²·æ­·å²å’Œåˆ†é¡...")
        progress_tracker.add_message("æ­£åœ¨æª¢æŸ¥å®¢æˆ¶è³¼è²·æ­·å²å’Œåˆ†é¡...")
        classified_results = classify_customers_by_purchase_status(product_name, all_results)
    
        # æŒ‰åˆ†æ•¸æ’åº
        all_results.sort(key=lambda x: x['match_score'], reverse=True)
        progress_tracker.add_message("å®¢æˆ¶åˆ†é¡å®Œæˆï¼Œæ­£åœ¨æ•´ç†åˆ†æçµæœ...")
        
        # æº–å‚™æª”æ¡ˆåç¨±ï¼ˆä½¿ç”¨ç›¸åŒçš„æ¸…ç†é‚è¼¯ï¼‰
        result_filename = f"{clean_product_name}_{timestamp}"
    
        # ä¿å­˜åˆ°å…©å€‹ä½ç½®çš„æª”æ¡ˆ
        progress_tracker.update_step(6, "ä¿å­˜çµæœ", "æ­£åœ¨ä¿å­˜åˆ†æçµæœå’Œæ›´æ–°ç´¢å¼•...")
        for folder in [date_folder, product_folder]:
            # JSON æª”æ¡ˆ
            json_file = folder / f"{result_filename}_{len(all_results)}_matches.json"
            print(f"æ­£åœ¨ä¿å­˜JSONæª”æ¡ˆ: {json_file.absolute()}")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'product_name': product_name,
                    'keywords_used': keywords,
                    'total_results': len(all_results),
                    'files_searched': total_files,
                    'search_timestamp': timestamp,
                    'search_date': date_str,
                    'search_type': 'complete_all_files_organized',
                    'classification_stats': classified_results['stats'],
                    'results': all_results
                }, f, ensure_ascii=False, indent=2, default=str)
            
            # CSV æª”æ¡ˆ
            csv_file = folder / f"{result_filename}_{len(all_results)}_matches.csv"
            with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                if all_results:
                    fieldnames = [
                        'ç”¢å“åç¨±', 'å®¢æˆ¶åç¨±', 'å®¢æˆ¶ID', 'è¨Šæ¯å…§å®¹', 'è¨Šæ¯æ—¥æœŸ',
                        'åŒ¹é…é—œéµè©', 'åŒ¹é…é¡å‹', 'åŒ¹é…åˆ†æ•¸', 'ä¾†æºæª”æ¡ˆ',
                        'æœ‰å®¢æˆ¶ID', 'è³¼è²·ç‹€æ…‹', 'å¯ä»¥è™•ç†'
                    ]
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    
                    for result in all_results:
                        writer.writerow({
                            'ç”¢å“åç¨±': result['product_name'],
                            'å®¢æˆ¶åç¨±': result['customer_name'],
                            'å®¢æˆ¶ID': result.get('customer_id', ''),
                            'è¨Šæ¯å…§å®¹': result['message_content'],
                            'è¨Šæ¯æ—¥æœŸ': result['date_str'] or '',
                            'åŒ¹é…é—œéµè©': result['matched_keyword'],
                            'åŒ¹é…é¡å‹': result['match_type'],
                            'åŒ¹é…åˆ†æ•¸': f"{result['match_score']:.2f}",
                            'ä¾†æºæª”æ¡ˆ': result['file_source'],
                            'æœ‰å®¢æˆ¶ID': 'æ˜¯' if result.get('has_customer_id', False) else 'å¦',
                            'è³¼è²·ç‹€æ…‹': result.get('purchase_status', 'unknown'),
                            'å¯ä»¥è™•ç†': 'æ˜¯' if result.get('can_process', False) else 'å¦'
                        })
            
            # é—œéµè©æª”æ¡ˆ
            keywords_file = folder / f"{result_filename}_keywords.json"
            with open(keywords_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'product_name': product_name,
                    'keywords': keywords,
                    'count': len(keywords),
                    'generated_at': timestamp
                }, f, ensure_ascii=False, indent=2)
        
            # å®¢æˆ¶åˆ†é¡æª”æ¡ˆ
            classification_file = folder / f"{result_filename}_classification.json"
            with open(classification_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'product_name': product_name,
                    'classification_timestamp': timestamp,
                    'stats': classified_results['stats'],
                    'can_process_customers': [
                        {
                            'customer_name': r['customer_name'],
                            'customer_id': r['customer_id'],
                            'matched_keyword': r['matched_keyword'],
                            'message_content': r['message_content'][:100] + '...' if len(r['message_content']) > 100 else r['message_content'],
                            'file_source': r['file_source']
                        }
                        for r in classified_results['can_process']
                    ],
                    'already_purchased_customers': [
                        {
                            'customer_name': r['customer_name'],
                            'customer_id': r['customer_id'],
                            'matched_keyword': r['matched_keyword'],
                            'file_source': r['file_source']
                        }
                        for r in classified_results['already_purchased']
                    ],
                    'cannot_process_customers': [
                        {
                            'customer_name': r['customer_name'],
                            'matched_keyword': r['matched_keyword'],
                            'reason': 'ç„¡å®¢æˆ¶ID',
                            'file_source': r['file_source']
                        }
                        for r in classified_results['cannot_process']
                    ]
                }, f, ensure_ascii=False, indent=2)
    
        # ç”Ÿæˆçµ±è¨ˆå ±å‘Šï¼ˆåªåœ¨æŒ‰æ—¥æœŸè³‡æ–™å¤¾ï¼‰
        stats_file = date_folder / f"{result_filename}_statistics.txt"
        generate_statistics_report(stats_file, product_name, timestamp, total_files, keywords, all_results, classified_results)
        
        # æ›´æ–°æœå°‹ç´¢å¼•
        index_data = load_search_index()
        
        search_entry = {
            'timestamp': timestamp,
            'date': date_str,
            'product_name': product_name,
            'keywords_count': len(keywords),
            'files_searched': total_files,
            'results_found': len(all_results),
            'date_folder': str(date_folder.name),
            'product_folder': str(product_folder.name),
            'json_file': f"{result_filename}_{len(all_results)}_matches.json",
            'csv_file': f"{result_filename}_{len(all_results)}_matches.csv"
        }
        
        index_data["searches"].append(search_entry)
        index_data["statistics"]["total_searches"] += 1
        index_data["statistics"]["total_matches"] += len(all_results)
        
        # æ›´æ–°ç”¢å“ç´¢å¼•
        if product_name not in index_data["products"]:
            index_data["products"][product_name] = []
        index_data["products"][product_name].append(search_entry)
        
        save_search_index(index_data)
        
        # æ–°å¢ï¼šå®¢æˆ¶æ•´åˆåˆ†æ - åªåˆ†æå¯ä»¥è™•ç†çš„å®¢æˆ¶å’Œå·²è³¼è²·çš„å®¢æˆ¶
        print("\næ­£åœ¨é€²è¡Œå®¢æˆ¶æ•´åˆåˆ†æ...")
        progress_tracker.update_step(7, "å®¢æˆ¶æ•´åˆåˆ†æ", "æ­£åœ¨é€²è¡Œæ·±åº¦å®¢æˆ¶æ•´åˆåˆ†æ...")
        
        # åˆä½µæ‰€æœ‰æ‰¾åˆ°çš„å®¢æˆ¶ä¾†é€²è¡Œå®Œæ•´çš„å®¢æˆ¶æ•´åˆåˆ†æ
        # åŒ…æ‹¬ï¼šå¯ä»¥è™•ç†çš„å®¢æˆ¶ã€å·²è³¼è²·çš„å®¢æˆ¶ã€ç„¡æ³•è™•ç†çš„å®¢æˆ¶ï¼ˆæ²’æœ‰customer_idä½†ä»æ˜¯æ½›åœ¨å®¢æˆ¶ï¼‰
        # ç‚ºæ¯å€‹å®¢æˆ¶æ·»åŠ ä¾†æºåˆ†é¡æ¨™è¨˜
        inquiry_customers_for_analysis = []
        
        # æ·»åŠ å¯è™•ç†å®¢æˆ¶ï¼ˆæ¨™è¨˜ç‚º can_processï¼‰
        for customer in classified_results['can_process']:
            customer['source_classification'] = 'can_process'
            inquiry_customers_for_analysis.append(customer)
            
        # æ·»åŠ å·²è³¼è²·å®¢æˆ¶ï¼ˆæ¨™è¨˜ç‚º already_purchasedï¼‰
        for customer in classified_results['already_purchased']:
            customer['source_classification'] = 'already_purchased'
            inquiry_customers_for_analysis.append(customer)
            
        # æ·»åŠ ç„¡æ³•è™•ç†å®¢æˆ¶ï¼ˆæ¨™è¨˜ç‚º cannot_processï¼‰
        for customer in classified_results['cannot_process']:
            customer['source_classification'] = 'cannot_process'
            inquiry_customers_for_analysis.append(customer)
        
        if inquiry_customers_for_analysis:
            # å¦‚æœæœ‰å®¢æˆ¶è³‡æ–™ï¼Œé€²è¡Œå®Œæ•´æ•´åˆåˆ†æ
            integration_result = generate_integrated_customer_analysis(
                product_name, inquiry_customers_for_analysis, date_folder, product_folder, result_filename, timestamp
            )
        else:
            # å¦‚æœæ²’æœ‰å®¢æˆ¶è³‡æ–™ï¼Œå‰µå»ºç©ºçš„æ•´åˆåˆ†ææª”æ¡ˆ
            print("æ²’æœ‰å®¢æˆ¶è³‡æ–™éœ€è¦æ•´åˆåˆ†æï¼Œå‰µå»ºç©ºçµæœæª”æ¡ˆ...")
            integrated_filename = f"{result_filename}_integrated_customers.json"
            empty_analysis_result = {
                "product_name": product_name,
                "analysis_timestamp": timestamp,
                "summary": {
                    "total_customers": 0,
                    "purchased_customers": 0,
                    "potential_customers": 0,
                    "inquiry_customers": 0
                },
                "customers": []
            }
            
            # ä¿å­˜ç©ºçµæœåˆ°å…©å€‹ä½ç½®
            for folder in [date_folder, product_folder]:
                integrated_file = folder / integrated_filename
                with open(integrated_file, 'w', encoding='utf-8') as f:
                    json.dump(empty_analysis_result, f, ensure_ascii=False, indent=2, default=str)
            
            integration_result = empty_analysis_result
        
        # å¦‚æœæœ‰èˆŠçš„æœªçµ„ç¹”æª”æ¡ˆï¼Œç§»å‹•åˆ°æ–°ç³»çµ±ä¸­
        cleanup_old_search_files(timestamp)
        
        print(f"\n" + "=" * 60)
        print(f"æœå°‹å®Œæˆï¼çµæœå·²çµ„ç¹”åŒ–ä¿å­˜")
        print(f"ç¸½åŒ¹é…æ•¸: {len(all_results)}")
        print(f"æŒ‰æ—¥æœŸè³‡æ–™å¤¾: {date_folder}")
        print(f"æŒ‰ç”¢å“è³‡æ–™å¤¾: {product_folder}")
        print(f"ç´¢å¼•å·²æ›´æ–°: {SEARCH_RESULTS_DIR}/reports/search_index.json")
        print("=" * 60)
        
        # æ§‹å»ºå®Œæ•´çš„APIè¿”å›çµæœ
        customer_analysis = []
        for customer in classified_results['can_process']:
            customer_data = {
                'customer_name': customer['customer_name'],
                'customer_id': customer['customer_id'],
                'customer_type': 'æ½›åœ¨éœ€æ±‚å®¢æˆ¶',
                'conversation_summary': customer['message_content'][:200] + '...' if len(customer['message_content']) > 200 else customer['message_content'],
                'last_purchase_date': '',
                'purchase_count': 0
            }
            customer_analysis.append(customer_data)
        
        # æº–å‚™æœ€çµ‚çµæœ
        final_result = {
            'date_folder': str(date_folder),
            'product_folder': str(product_folder),
            'results_count': len(all_results),
            'customer_analysis': customer_analysis,
            'classification_stats': classified_results['stats'],
            'can_process_customers': classified_results['can_process'],
            'purchased_customers': classified_results['already_purchased'],
            'potential_customers': classified_results.get('potential_customers', []),
            'cannot_process_customers': classified_results['cannot_process'],
            'files': {
                'json': f"{result_filename}_{len(all_results)}_matches.json",
                'csv': f"{result_filename}_{len(all_results)}_matches.csv",
                'keywords': f"{result_filename}_keywords.json",
                'statistics': f"{result_filename}_statistics.txt",
                'integrated_customers': f"{result_filename}_integrated_customers.json"
            }
        }
        
        # å®Œæˆä»»å‹™ä¸¦è¨­ç½®é€²åº¦
        progress_tracker.complete_task(len(all_results), final_result)
        
        return final_result
        
    except Exception as e:
        # éŒ¯èª¤è™•ç†
        logger.error(f"æ½›åœ¨å®¢æˆ¶åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        
        # æ¨™è¨˜ä»»å‹™éŒ¯èª¤
        progress_tracker.error_task(str(e))
        
        # é‡æ–°æ‹‹å‡ºç•°å¸¸è®“ä¸Šå±¤è™•ç†
        raise

def generate_statistics_report(stats_file, product_name, timestamp, total_files, keywords, all_results, classified_results):
    """ç”Ÿæˆçµ±è¨ˆå ±å‘Š"""
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("æœå°‹çµ±è¨ˆå ±å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"ç”¢å“åç¨±: {product_name}\n")
        f.write(f"æœå°‹æ™‚é–“: {timestamp}\n")
        f.write(f"æœå°‹æª”æ¡ˆæ•¸: {total_files}\n")
        f.write(f"ä½¿ç”¨é—œéµè©æ•¸: {len(keywords)}\n")
        f.write(f"ç¸½åŒ¹é…æ•¸: {len(all_results)}\n\n")
        
        # å®¢æˆ¶åˆ†é¡çµ±è¨ˆ
        stats = classified_results['stats']
        f.write("å®¢æˆ¶åˆ†é¡çµ±è¨ˆ:\n")
        f.write("-" * 40 + "\n")
        f.write(f"  ç¸½çµæœæ•¸: {stats['total_results']}\n")
        f.write(f"  æœ‰å®¢æˆ¶ID: {stats['has_customer_id']} å€‹\n")
        f.write(f"  ç„¡å®¢æˆ¶ID: {stats['no_customer_id']} å€‹\n")
        f.write(f"  å·²è³¼è²·è©²ç”¢å“: {stats['purchased_count']} å€‹\n")
        f.write(f"  æœªè³¼è²·è©²ç”¢å“: {stats['not_purchased_count']} å€‹\n")
        f.write(f"  ğŸ¯ å¯ä»¥è™•ç†çš„æ½›åœ¨å®¢æˆ¶: {stats['can_process_count']} å€‹\n")
        f.write(f"  âŒ ä¸èƒ½è™•ç†çš„å®¢æˆ¶: {stats['cannot_process_count']} å€‹\n\n")
        
        # æŒ‰é—œéµè©åˆ†çµ„çµ±è¨ˆ
        keyword_stats = {}
        for result in all_results:
            keyword = result['matched_keyword']
            keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1
        
        f.write("é—œéµè©åŒ¹é…åˆ†å¸ƒ:\n")
        f.write("-" * 40 + "\n")
        for keyword, count in sorted(keyword_stats.items(), key=lambda x: x[1], reverse=True):
            f.write(f"  '{keyword}': {count} æ¬¡\n")
        
        # å¯ä»¥è™•ç†çš„å®¢æˆ¶çµ±è¨ˆ
        processable_customers = {}
        for result in classified_results['can_process']:
            customer = result['customer_name']
            processable_customers[customer] = processable_customers.get(customer, 0) + 1
        
        f.write("\nğŸ¯ å¯ä»¥è™•ç†çš„æ½›åœ¨å®¢æˆ¶ TOP 20:\n")
        f.write("-" * 40 + "\n")
        for customer, count in sorted(processable_customers.items(), key=lambda x: x[1], reverse=True)[:20]:
            f.write(f"  {customer}: {count} æ¬¡\n")
        
        # å·²è³¼è²·å®¢æˆ¶çµ±è¨ˆ
        purchased_customers = {}
        for result in classified_results['already_purchased']:
            customer = result['customer_name']
            purchased_customers[customer] = purchased_customers.get(customer, 0) + 1
        
        if purchased_customers:
            f.write("\nâŒ å·²è³¼è²·è©²ç”¢å“çš„å®¢æˆ¶ TOP 10:\n")
            f.write("-" * 40 + "\n")
            for customer, count in sorted(purchased_customers.items(), key=lambda x: x[1], reverse=True)[:10]:
                f.write(f"  {customer}: {count} æ¬¡\n")
        
        # ç„¡æ³•è™•ç†çš„å®¢æˆ¶çµ±è¨ˆ
        unprocessable_customers = {}
        for result in classified_results['cannot_process']:
            customer = result['customer_name']
            unprocessable_customers[customer] = unprocessable_customers.get(customer, 0) + 1
        
        if unprocessable_customers:
            f.write("\nâš ï¸ ç„¡æ³•è™•ç†çš„å®¢æˆ¶ï¼ˆç„¡å®¢æˆ¶IDï¼‰TOP 10:\n")
            f.write("-" * 40 + "\n")
            for customer, count in sorted(unprocessable_customers.items(), key=lambda x: x[1], reverse=True)[:10]:
                f.write(f"  {customer}: {count} æ¬¡\n")

def cleanup_old_search_files(current_timestamp):
    """æ¸…ç†èˆŠçš„æœªçµ„ç¹”æœå°‹æª”æ¡ˆï¼Œç§»å‹•åˆ°æ–°ç³»çµ±"""
    current_dir = Path(".")
    old_files = list(current_dir.glob("search_results_*.json")) + list(current_dir.glob("full_search_results_*.json"))
    
    if old_files:
        print(f"\nğŸ§¹ ç™¼ç¾ {len(old_files)} å€‹æœªçµ„ç¹”çš„æœå°‹æª”æ¡ˆï¼Œæ­£åœ¨ç§»å‹•...")
        
        # å‰µå»ºèˆŠæª”æ¡ˆå‚™ä»½è³‡æ–™å¤¾
        archive_folder = SEARCH_RESULTS_DIR / "archived_searches"
        archive_folder.mkdir(exist_ok=True)
        
        for old_file in old_files:
            if current_timestamp not in old_file.name:  # ä¸ç§»å‹•ç•¶å‰ç”¢ç”Ÿçš„æª”æ¡ˆ
                try:
                    shutil.move(str(old_file), str(archive_folder / old_file.name))
                    print(f"   âœ… å·²ç§»å‹•: {old_file.name}")
                except Exception as e:
                    print(f"   âš ï¸ ç„¡æ³•ç§»å‹• {old_file.name}: {e}")

def show_search_history():
    """é¡¯ç¤ºæœå°‹æ­·å²"""
    if not (SEARCH_RESULTS_DIR / "reports" / "search_index.json").exists():
        print("[HISTORY] å°šç„¡æœå°‹æ­·å²è¨˜éŒ„")
        return
    
    index_data = load_search_index()
    searches = index_data["searches"]
    
    if not searches:
        print("[HISTORY] å°šç„¡æœå°‹æ­·å²è¨˜éŒ„")
        return
    
    print(f"\n[HISTORY] æœå°‹æ­·å²ç¸½è¦½ (å…± {len(searches)} æ¬¡æœå°‹)")
    print("=" * 80)
    
    for i, search in enumerate(reversed(searches[-10:]), 1):  # é¡¯ç¤ºæœ€è¿‘10æ¬¡
        print(f"{i}. [{search['date']}] {search['product_name']}")
        print(f"   [RESULTS] {search['results_found']} å€‹çµæœ | [FOLDER] {search['date_folder']}")
        print()

def classify_customers_by_purchase_status(product_name: str, results: list):
    """
    æ ¹æ“šè³¼è²·ç‹€æ…‹åˆ†é¡å®¢æˆ¶
    
    Args:
        product_name: ç”¢å“åç¨±
        results: æœå°‹çµæœåˆ—è¡¨
        
    Returns:
        dict: åŒ…å«åˆ†é¡çµ±è¨ˆçš„å­—å…¸
    """
    # åˆå§‹åŒ–åˆ†é¡çµ±è¨ˆ
    classification = {
        'can_process': [],      # å¯ä»¥è™•ç†çš„å®¢æˆ¶ï¼ˆæœ‰customer_idä¸”æœªè³¼è²·ï¼‰
        'cannot_process': [],   # ä¸èƒ½è™•ç†çš„å®¢æˆ¶ï¼ˆç„¡customer_idï¼‰
        'already_purchased': [],  # å·²è³¼è²·çš„å®¢æˆ¶
        'not_purchased': [],    # æœªè³¼è²·çš„å®¢æˆ¶
        'stats': {
            'total_results': len(results),
            'has_customer_id': 0,
            'no_customer_id': 0,
            'purchased_count': 0,
            'not_purchased_count': 0,
            'can_process_count': 0,
            'cannot_process_count': 0
        }
    }
    
    # åˆ†é›¢æœ‰ç„¡customer_idçš„çµæœ
    results_with_id = []
    results_without_id = []
    customer_ids_map = {}
    
    for result in results:
        customer_id = result.get('customer_id')
        if customer_id:
            results_with_id.append(result)
            if customer_id not in customer_ids_map:
                customer_ids_map[customer_id] = []
            customer_ids_map[customer_id].append(result)
        else:
            # ç„¡customer_idçš„çµæœï¼Œç„¡æ³•é©—è­‰è³¼è²·æ­·å²
            result['has_customer_id'] = False
            result['purchase_status'] = 'unknown'
            result['can_process'] = False
            results_without_id.append(result)
            classification['cannot_process'].append(result)
    
    # æ›´æ–°çµ±è¨ˆ
    classification['stats']['has_customer_id'] = len(results_with_id)
    classification['stats']['no_customer_id'] = len(results_without_id)
    classification['stats']['cannot_process_count'] = len(results_without_id)
    
    # æ‰¹é‡æª¢æŸ¥è³¼è²·æ­·å² - ä½¿ç”¨å­é¡åˆ¥åŒ¹é…
    if customer_ids_map:
        db_manager = get_database_manager()
        customer_ids = list(customer_ids_map.keys())
        purchased_customer_ids = db_manager.get_customers_who_purchased_by_subcategory(product_name, customer_ids)
        
        # æ¨™è¨˜è³¼è²·ç‹€æ…‹ä¸¦åˆ†é¡
        for result in results_with_id:
            customer_id = result['customer_id']
            result['has_customer_id'] = True
            
            if customer_id in purchased_customer_ids:
                result['purchase_status'] = 'purchased'
                result['can_process'] = False  # å·²è³¼è²·ï¼Œä¸éœ€è¦è™•ç†
                classification['already_purchased'].append(result)
            else:
                result['purchase_status'] = 'not_purchased'
                result['can_process'] = True   # æœªè³¼è²·ï¼Œå¯ä»¥è™•ç†
                classification['not_purchased'].append(result)
                classification['can_process'].append(result)
        
        # æ›´æ–°çµ±è¨ˆ - ä¿®æ­£ï¼šçµ±è¨ˆå¯¦éš›åˆ†é¡çš„è¨˜éŒ„æ•¸è€Œä¸æ˜¯å”¯ä¸€å®¢æˆ¶æ•¸
        classification['stats']['purchased_count'] = len(classification['already_purchased'])
        classification['stats']['not_purchased_count'] = len(classification['not_purchased'])
        classification['stats']['can_process_count'] = len(classification['can_process'])
        
        # é¡¯ç¤ºåˆ†é¡çµ±è¨ˆ
        print("å®¢æˆ¶åˆ†é¡çµ±è¨ˆ:")
        print(f"  ç¸½çµæœæ•¸: {classification['stats']['total_results']}")
        print(f"  æœ‰customer_id: {classification['stats']['has_customer_id']} å€‹")
        print(f"  ç„¡customer_id: {classification['stats']['no_customer_id']} å€‹")
        print(f"  å·²è³¼è²·è©²ç”¢å“: {classification['stats']['purchased_count']} å€‹")
        print(f"  æœªè³¼è²·è©²ç”¢å“: {classification['stats']['not_purchased_count']} å€‹")
        print(f"  å¯ä»¥è™•ç†çš„æ½›åœ¨å®¢æˆ¶: {classification['stats']['can_process_count']} å€‹")
        print(f"  ä¸èƒ½è™•ç†çš„å®¢æˆ¶: {classification['stats']['cannot_process_count']} å€‹")
        
    return classification

def generate_integrated_customer_analysis(product_name: str, all_results: List[Dict], 
                                         date_folder: Path, product_folder: Path, 
                                         result_filename: str, timestamp: str) -> Dict:
    """
    ç”Ÿæˆå®¢æˆ¶æ•´åˆåˆ†æä¸¦ä¿å­˜çµ±ä¸€æ ¼å¼çš„JSONæª”æ¡ˆ
    
    Args:
        product_name: ç”¢å“åç¨±
        all_results: èŠå¤©è¨˜éŒ„æœå°‹çµæœ
        date_folder: æŒ‰æ—¥æœŸåˆ†çµ„çš„è³‡æ–™å¤¾
        product_folder: æŒ‰ç”¢å“åˆ†çµ„çš„è³‡æ–™å¤¾
        result_filename: çµæœæª”æ¡ˆåç¨±å‰ç¶´
        timestamp: æ™‚é–“æˆ³è¨˜
        
    Returns:
        Dict: æ•´åˆåˆ†æçµæœ
    """
    try:
        # ç²å–å®¢æˆ¶æ•´åˆåˆ†æå™¨
        integration_analyzer = get_customer_integration_analyzer()
        
        # åŸ·è¡Œæ•´åˆåˆ†æ - å‚³å…¥å·²ç¶“åˆ†é¡å¥½çš„æ‰€æœ‰å®¢æˆ¶
        analysis_result = integration_analyzer.analyze_all_customer_types(product_name, all_results)
        
        # æº–å‚™çµ±ä¸€æ ¼å¼çš„JSONæª”æ¡ˆåç¨±
        integrated_filename = f"{result_filename}_integrated_customers.json"
        
        # ä¿å­˜åˆ°å…©å€‹ä½ç½®
        for folder in [date_folder, product_folder]:
            integrated_file = folder / integrated_filename
            
            with open(integrated_file, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2, default=str)
        
        # é¡¯ç¤ºæ•´åˆåˆ†æçµæœæ‘˜è¦
        summary = analysis_result['summary']
        print("å®¢æˆ¶æ•´åˆåˆ†æçµæœ:")
        print(f"  ç¸½å®¢æˆ¶æ•¸: {summary['total_customers']} å€‹")
        print(f"  æ›¾è³¼è²·å®¢æˆ¶: {summary['purchased_customers']} å€‹")
        print(f"  æ½›åœ¨éœ€æ±‚å®¢æˆ¶: {summary['potential_customers']} å€‹")
        print(f"  æ›¾è©¢å•å®¢æˆ¶: {summary['inquiry_customers']} å€‹")
        
        # é¡¯ç¤ºå‰å¹¾å€‹å®¢æˆ¶ä½œç‚ºç¯„ä¾‹
        if analysis_result['customers']:
            print(f"\nå®¢æˆ¶ç¯„ä¾‹ (å‰5å€‹):")
            for i, customer in enumerate(analysis_result['customers'][:5], 1):
                customer_type = customer['customer_type']
                last_date = customer.get('last_activity_date', 'N/A')
                print(f"  {i}. {customer['customer_name']} ({customer_type}) - {last_date}")
        
        print(f"æ•´åˆåˆ†ææª”æ¡ˆå·²ä¿å­˜: {integrated_filename}")
        
        return analysis_result
        
    except Exception as e:
        print(f"å®¢æˆ¶æ•´åˆåˆ†æå¤±æ•—: {e}")
        logger.error(f"å®¢æˆ¶æ•´åˆåˆ†æå¤±æ•—: {e}")
        return {}

if __name__ == "__main__":
    # é¡¯ç¤ºæœå°‹æ­·å²
    show_search_history()
    
    # æ¥å—å‘½ä»¤è¡Œåƒæ•¸æˆ–æç¤ºç”¨æˆ¶è¼¸å…¥
    import sys
    
    if len(sys.argv) > 1:
        # ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸
        product_name = sys.argv[1]
        print(f"ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸æœå°‹ç”¢å“: {product_name}")
    else:
        # æç¤ºç”¨æˆ¶è¼¸å…¥
        product_name = input("\nè«‹è¼¸å…¥è¦æœå°‹çš„ç”¢å“åç¨±: ").strip()
        
        if not product_name:
            print("æœªæä¾›ç”¢å“åç¨±ï¼Œä½¿ç”¨é è¨­ç”¢å“é€²è¡Œæ¸¬è©¦...")
            product_name = "ç™½å¸¶é­šåˆ‡å¡Š3/4(æœ‰è‚š) 10K/ç®±-æ´¥æ¹§"
    
    # åŸ·è¡Œçµ„ç¹”åŒ–æœå°‹
    result = organized_complete_search(product_name)
    print(f"\nğŸ‰ æœå°‹å®Œæˆï¼çµæœå·²çµ„ç¹”åŒ–ä¿å­˜åˆ°å°ˆé–€çš„ç®¡ç†ç³»çµ±ä¸­")